
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Algorithmic Differentiation &#8212; Owl Numerical Library 0.3 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Optimisation Engine" href="optimise.html" />
    <link rel="prev" title="Lazy Evaluation and Dataflow" href="lazy.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="../index.html">
    <img class="logo" src="../_static/owl_logo_4.png" alt="Logo"/>
    
  </a>
</p>











  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Algorithmic Differentiation</a><ul>
<li><a class="reference internal" href="#high-level-apis">High-level APIs</a></li>
<li><a class="reference internal" href="#forward-or-backward">Forward or Backward?</a></li>
<li><a class="reference internal" href="#examples">Examples</a><ul>
<li><a class="reference internal" href="#example-1-higher-order-derivatives">Example 1: Higher-Order Derivatives</a></li>
<li><a class="reference internal" href="#example-2-gradient-descent-algorithm">Example 2: Gradient Descent Algorithm</a></li>
<li><a class="reference internal" href="#example-3-newton-s-algorithm">Example 3: Newton’s Algorithm</a></li>
<li><a class="reference internal" href="#example-4-backpropagation-in-neural-network">Example 4: Backpropagation in Neural Network</a></li>
<li><a class="reference internal" href="#example-5-computation-graph-of-simple-functions">Example 5: Computation Graph of Simple Functions</a></li>
<li><a class="reference internal" href="#example-6-computation-graph-of-vgg-like-neural-network">Example 6: Computation Graph of VGG-like Neural Network</a></li>
<li><a class="reference internal" href="#example-7-computation-graph-of-lstm-network">Example 7: Computation Graph of LSTM Network</a></li>
<li><a class="reference internal" href="#example-8-computation-graph-of-google-s-inception">Example 8: Computation Graph of Google’s Inception</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">Part II: Owl Numerical System</a><ul>
      <li>Previous: <a href="lazy.html" title="previous chapter">Lazy Evaluation and Dataflow</a></li>
      <li>Next: <a href="optimise.html" title="next chapter">Optimisation Engine</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="algorithmic-differentiation">
<h1>Algorithmic Differentiation<a class="headerlink" href="#algorithmic-differentiation" title="Permalink to this headline">¶</a></h1>
<p>Algorithmic differentiation (AD) is also known as automatic differentiation. It is a powerful tool in many fields, especially useful for fast prototyping in machine learning research. Comparing to numerical differentiation which can only provides approximate results, AD can calculates the exact derivative of a given function.</p>
<p>Owl provides both numerical differentiation (in <a class="reference external" href="https://github.com/ryanrhymes/owl/blob/ppl/src/base/optimise/owl_numdiff_generic.mli">Numdiff.Generic</a> module) and algorithmic differentiation (in <a class="reference external" href="https://github.com/ryanrhymes/owl/blob/ppl/src/base/optimise/owl_algodiff_generic.mli">Algodiff.Generic</a> module). In this chapter, I will only go through AD module since <code class="docutils literal notranslate"><span class="pre">Numerical</span></code> module is trivial to use.</p>
<div class="section" id="high-level-apis">
<h2>High-level APIs<a class="headerlink" href="#high-level-apis" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Algodiff.Generic</span></code> is a functor which is able to support both <code class="docutils literal notranslate"><span class="pre">float32</span></code> and <code class="docutils literal notranslate"><span class="pre">float64</span></code> precision <code class="docutils literal notranslate"><span class="pre">AD</span></code>. However, you do not need to deal with <code class="docutils literal notranslate"><span class="pre">Algodiff.Generic.Make</span></code> directly since there are already two ready-made modules.</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">Algodiff.S</span></code> supports <code class="docutils literal notranslate"><span class="pre">float32</span></code> precision;</li>
<li><code class="docutils literal notranslate"><span class="pre">Algodiff.D</span></code> supports <code class="docutils literal notranslate"><span class="pre">float64</span></code> precision;</li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">Algodiff</span></code> has implemented both forward and backward mode of AD. The complete list of APIs can be found in <a class="reference external" href="https://github.com/ryanrhymes/owl/blob/ppl/src/base/optimise/owl_algodiff_generic.mli">owl_algodiff_generic.mli</a>. The core APIs are listed below.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">val</span> <span class="n">diff</span> <span class="o">:</span> <span class="o">(</span><span class="n">t</span> <span class="o">-&gt;</span> <span class="n">t</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">t</span> <span class="o">-&gt;</span> <span class="n">t</span>
<span class="c">(* calculate derivative for f : scalar -&gt; scalar *)</span>

<span class="k">val</span> <span class="n">grad</span> <span class="o">:</span> <span class="o">(</span><span class="n">t</span> <span class="o">-&gt;</span> <span class="n">t</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">t</span> <span class="o">-&gt;</span> <span class="n">t</span>
<span class="c">(* calculate gradient for f : vector -&gt; scalar *)</span>

<span class="k">val</span> <span class="n">jacobian</span> <span class="o">:</span> <span class="o">(</span><span class="n">t</span> <span class="o">-&gt;</span> <span class="n">t</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">t</span> <span class="o">-&gt;</span> <span class="n">t</span>
<span class="c">(* calculate jacobian for f : vector -&gt; vector *)</span>

<span class="k">val</span> <span class="n">hessian</span> <span class="o">:</span> <span class="o">(</span><span class="n">t</span> <span class="o">-&gt;</span> <span class="n">t</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">t</span> <span class="o">-&gt;</span> <span class="n">t</span>
<span class="c">(* calculate hessian for f : scalar -&gt; scalar *)</span>

<span class="k">val</span> <span class="n">laplacian</span> <span class="o">:</span> <span class="o">(</span><span class="n">t</span> <span class="o">-&gt;</span> <span class="n">t</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">t</span> <span class="o">-&gt;</span> <span class="n">t</span>
<span class="c">(* calculate laplacian for f : scalar -&gt; scalar *)</span>
</pre></div>
</div>
<p>Besides, there are also more helper functions such as <code class="docutils literal notranslate"><span class="pre">jacobianv</span></code> for calculating jacobian vector product; <code class="docutils literal notranslate"><span class="pre">diff'</span></code> for calculating both <code class="docutils literal notranslate"><span class="pre">f</span> <span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">diff</span> <span class="pre">f</span> <span class="pre">x</span></code>, and etc.</p>
</div>
<div class="section" id="forward-or-backward">
<h2>Forward or Backward?<a class="headerlink" href="#forward-or-backward" title="Permalink to this headline">¶</a></h2>
<p>There are two modes in algorithmic differentiation - forward mode and backward mode. Owl has implemented both. Since both can be used to differentiate a function then the natural question is which mode we should choose in practice. The short answer is: it depends on your function :)</p>
<p>In general, given a function that you want to differentiate, the rule of thumb is:</p>
<ul class="simple">
<li>if input variables &gt;&gt; output variables, then use backward mode;</li>
<li>if input variables &lt;&lt; output variables, then use forward mode.</li>
</ul>
<p>E.g., let’s look at the two simple functions <code class="docutils literal notranslate"><span class="pre">f</span></code> and <code class="docutils literal notranslate"><span class="pre">g</span></code> defined below. <code class="docutils literal notranslate"><span class="pre">f</span></code> falls into the first category we mentioned before, i.e., inputs is more than outputs; whilst <code class="docutils literal notranslate"><span class="pre">g</span></code> falls into the second category.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">open</span> <span class="nn">Algodiff</span><span class="p">.</span><span class="nc">D</span><span class="o">;;</span>

<span class="c">(* f : vector -&gt; scalar *)</span>
<span class="k">let</span> <span class="n">f</span> <span class="n">x</span> <span class="o">=</span>
  <span class="k">let</span> <span class="n">a</span> <span class="o">=</span> <span class="nn">Mat</span><span class="p">.</span><span class="n">get</span> <span class="n">x</span> <span class="mi">0</span> <span class="mi">0</span> <span class="k">in</span>
  <span class="k">let</span> <span class="n">b</span> <span class="o">=</span> <span class="nn">Mat</span><span class="p">.</span><span class="n">get</span> <span class="n">x</span> <span class="mi">0</span> <span class="mi">1</span> <span class="k">in</span>
  <span class="k">let</span> <span class="n">c</span> <span class="o">=</span> <span class="nn">Mat</span><span class="p">.</span><span class="n">get</span> <span class="n">x</span> <span class="mi">0</span> <span class="mi">2</span> <span class="k">in</span>
  <span class="nn">Maths</span><span class="p">.</span><span class="o">((</span><span class="n">sin</span> <span class="n">a</span><span class="o">)</span> <span class="o">+</span> <span class="o">(</span><span class="n">cos</span> <span class="n">b</span><span class="o">)</span> <span class="o">+</span> <span class="o">(</span><span class="n">sqr</span> <span class="n">c</span><span class="o">))</span>
<span class="o">;;</span>

<span class="c">(* g : scalar -&gt; vector *)</span>
<span class="k">let</span> <span class="n">g</span> <span class="n">x</span> <span class="o">=</span>
  <span class="k">let</span> <span class="n">a</span> <span class="o">=</span> <span class="nn">Maths</span><span class="p">.</span><span class="n">sin</span> <span class="n">x</span> <span class="k">in</span>
  <span class="k">let</span> <span class="n">b</span> <span class="o">=</span> <span class="nn">Maths</span><span class="p">.</span><span class="n">cos</span> <span class="n">x</span> <span class="k">in</span>
  <span class="k">let</span> <span class="n">c</span> <span class="o">=</span> <span class="nn">Maths</span><span class="p">.</span><span class="n">sqr</span> <span class="n">x</span> <span class="k">in</span>

  <span class="k">let</span> <span class="n">y</span> <span class="o">=</span> <span class="nn">Mat</span><span class="p">.</span><span class="n">zeros</span> <span class="mi">1</span> <span class="mi">3</span> <span class="k">in</span>
  <span class="k">let</span> <span class="n">y</span> <span class="o">=</span> <span class="nn">Mat</span><span class="p">.</span><span class="n">set</span> <span class="n">y</span> <span class="mi">0</span> <span class="mi">0</span> <span class="n">a</span> <span class="k">in</span>
  <span class="k">let</span> <span class="n">y</span> <span class="o">=</span> <span class="nn">Mat</span><span class="p">.</span><span class="n">set</span> <span class="n">y</span> <span class="mi">0</span> <span class="mi">1</span> <span class="n">b</span> <span class="k">in</span>
  <span class="k">let</span> <span class="n">y</span> <span class="o">=</span> <span class="nn">Mat</span><span class="p">.</span><span class="n">set</span> <span class="n">y</span> <span class="mi">0</span> <span class="mi">2</span> <span class="n">c</span> <span class="k">in</span>
  <span class="n">y</span>
<span class="o">;;</span>
</pre></div>
</div>
<p>According to the rule of thumb, we need to use backward mode to differentiate <code class="docutils literal notranslate"><span class="pre">f</span></code>, i.e., calculate the gradient of <code class="docutils literal notranslate"><span class="pre">f</span></code>. How to do that then? Let’s look at the code snippet below.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">let</span> <span class="n">x</span> <span class="o">=</span> <span class="nn">Mat</span><span class="p">.</span><span class="n">uniform</span> <span class="mi">1</span> <span class="mi">3</span><span class="o">;;</span>           <span class="c">(* generate random input *)</span>
<span class="k">let</span> <span class="n">x&#39;</span> <span class="o">=</span> <span class="n">make_reverse</span> <span class="n">x</span> <span class="o">(</span><span class="n">tag</span> <span class="bp">()</span><span class="o">);;</span>  <span class="c">(* init the backward mode *)</span>
<span class="k">let</span> <span class="n">y</span> <span class="o">=</span> <span class="n">f</span> <span class="n">x&#39;</span><span class="o">;;</span>                      <span class="c">(* forward pass to build computation graph *)</span>
<span class="n">reverse_prop</span> <span class="o">(</span><span class="nc">F</span> <span class="mi">1</span><span class="o">.)</span> <span class="n">y</span><span class="o">;;</span>             <span class="c">(* backward pass to propagate error *)</span>
<span class="k">let</span> <span class="n">y&#39;</span> <span class="o">=</span> <span class="n">adjval</span> <span class="n">x&#39;</span><span class="o">;;</span>                <span class="c">(* get the gradient value of f *)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">make_reverse</span></code> function does two things for us: 1) wrap <code class="docutils literal notranslate"><span class="pre">x</span></code> into type <code class="docutils literal notranslate"><span class="pre">t</span></code> that Algodiff can process using type constructor <code class="docutils literal notranslate"><span class="pre">DF</span></code>; 2) generate a unique tag for the input so that input numbers can have nested structure. By calling <code class="docutils literal notranslate"><span class="pre">f</span> <span class="pre">x'</span></code>, we construct the computation graph of <code class="docutils literal notranslate"><span class="pre">f</span></code> and the graph structure is maintained in the returned result <code class="docutils literal notranslate"><span class="pre">y</span></code>. Finally, <code class="docutils literal notranslate"><span class="pre">reverse_prop</span></code> function propagates the error back to the inputs.</p>
<p>In the end, the gradient of <code class="docutils literal notranslate"><span class="pre">f</span></code> is stored in the adjacent value of <code class="docutils literal notranslate"><span class="pre">x'</span></code>, and we can retrieve that with <code class="docutils literal notranslate"><span class="pre">adjval</span></code> function.</p>
<p>How about function <code class="docutils literal notranslate"><span class="pre">g</span></code> then, the function represents those having a small amount of inputs but a large amount of outputs. According to the rule of thumb, we are suppose to use the forward pass to calculate the derivatives of the outputs w.r.t its inputs.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">let</span> <span class="n">x</span> <span class="o">=</span> <span class="n">make_forward</span> <span class="o">(</span><span class="nc">F</span> <span class="mi">1</span><span class="o">.)</span> <span class="o">(</span><span class="nc">F</span> <span class="mi">1</span><span class="o">.)</span> <span class="o">(</span><span class="n">tag</span> <span class="bp">()</span><span class="o">);;</span>  <span class="c">(* seed the input *)</span>
<span class="k">let</span> <span class="n">y</span> <span class="o">=</span> <span class="n">g</span> <span class="n">x</span><span class="o">;;</span>                                  <span class="c">(* forward pass *)</span>
<span class="k">let</span> <span class="n">y&#39;</span> <span class="o">=</span> <span class="n">tangent</span> <span class="n">y</span><span class="o">;;</span>                           <span class="c">(* get all derivatives *)</span>
</pre></div>
</div>
<p>Forward mode appears much simpler than the backward mode. <code class="docutils literal notranslate"><span class="pre">make_forward</span></code> function does almost the same thing as <code class="docutils literal notranslate"><span class="pre">make_reverse</span></code> does for us, the only exception is that <code class="docutils literal notranslate"><span class="pre">make_forward</span></code> uses <code class="docutils literal notranslate"><span class="pre">DF</span></code> type constructor to wrap up the input. All the derivatives are ready whenever the forward pass is finished, and they are stored as tangent values in <code class="docutils literal notranslate"><span class="pre">y</span></code>. We can retrieve the derivatives using <code class="docutils literal notranslate"><span class="pre">tangent</span></code> function, as we used <code class="docutils literal notranslate"><span class="pre">adjval</span></code> in the backward mode.</p>
<p>OK, how about we abandon the rule of thumb? In other words, let’s use forward mode to differentiate <code class="docutils literal notranslate"><span class="pre">f</span></code> rather than using backward mode. Please check the solution below.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">let</span> <span class="n">x0</span> <span class="o">=</span> <span class="n">make_forward</span> <span class="n">x</span> <span class="o">(</span><span class="nc">Mat</span> <span class="nn">Vec</span><span class="p">.</span><span class="o">(</span><span class="n">unit_basis</span> <span class="mi">3</span> <span class="mi">0</span><span class="o">))</span> <span class="o">(</span><span class="n">tag</span> <span class="bp">()</span><span class="o">);;</span>  <span class="c">(* seed the first input variable *)</span>
<span class="k">let</span> <span class="n">t0</span> <span class="o">=</span> <span class="n">tangent</span> <span class="o">(</span><span class="n">f</span> <span class="n">x0</span><span class="o">);;</span>                                      <span class="c">(* forward pass for the first variable *)</span>

<span class="k">let</span> <span class="n">x1</span> <span class="o">=</span> <span class="n">make_forward</span> <span class="n">x</span> <span class="o">(</span><span class="nc">Mat</span> <span class="nn">Vec</span><span class="p">.</span><span class="o">(</span><span class="n">unit_basis</span> <span class="mi">3</span> <span class="mi">1</span><span class="o">))</span> <span class="o">(</span><span class="n">tag</span> <span class="bp">()</span><span class="o">);;</span>  <span class="c">(* seed the second input variable *)</span>
<span class="k">let</span> <span class="n">t1</span> <span class="o">=</span> <span class="n">tangent</span> <span class="o">(</span><span class="n">f</span> <span class="n">x1</span><span class="o">);;</span>                                      <span class="c">(* forward pass for the second variable *)</span>

<span class="k">let</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">make_forward</span> <span class="n">x</span> <span class="o">(</span><span class="nc">Mat</span> <span class="nn">Vec</span><span class="p">.</span><span class="o">(</span><span class="n">unit_basis</span> <span class="mi">3</span> <span class="mi">2</span><span class="o">))</span> <span class="o">(</span><span class="n">tag</span> <span class="bp">()</span><span class="o">);;</span>  <span class="c">(* seed the third input variable *)</span>
<span class="k">let</span> <span class="n">t2</span> <span class="o">=</span> <span class="n">tangent</span> <span class="o">(</span><span class="n">f</span> <span class="n">x2</span><span class="o">);;</span>                                      <span class="c">(* forward pass for the third variable *)</span>
</pre></div>
</div>
<p>As we can see, for each input variable, we need to seed individual variable and perform one forward pass. The number of forward passes increase linearly as the number of inputs increases. However, for backward mode, no matter how many inputs there are, one backward pass can give us all the derivatives of the inputs. I guess now you understand why we need to use backward mode for <code class="docutils literal notranslate"><span class="pre">f</span></code>. One real-world example of <code class="docutils literal notranslate"><span class="pre">f</span></code> is machine learning and neural network algorithms, wherein there are many inputs but the output is often one scalar value from loss function.</p>
<p>Similarly, you can try to use backward mode to differentiate <code class="docutils literal notranslate"><span class="pre">g</span></code>. I will just this as an exercise for you. One last thing I want to mention is: backward mode needs to maintain a directed computation graph in the memory so that the errors can propagate back; whereas the forward mode does not have to do that due to the algebra of dual numbers.</p>
<p>In reality, you don’t really need to worry about forward or backward mode if you simply use high-level APIs such as <code class="docutils literal notranslate"><span class="pre">diff</span></code>, <code class="docutils literal notranslate"><span class="pre">grad</span></code>, <code class="docutils literal notranslate"><span class="pre">hessian</span></code>, and etc. However, there might be cases you do need to operate these low-level functions to write up your own applications (e.g., implementing a neural network), then knowing the mechanisms behind the scene is definitely a big plus.</p>
</div>
<div class="section" id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h2>
<p>In order to understand AD, you need to practice enough, especially if you are interested in the knowing the mechanisms under the hood. I provide some small but representative examples to help you start.</p>
<div class="section" id="example-1-higher-order-derivatives">
<h3>Example 1: Higher-Order Derivatives<a class="headerlink" href="#example-1-higher-order-derivatives" title="Permalink to this headline">¶</a></h3>
<p>The following code first defines a function <code class="docutils literal notranslate"><span class="pre">f0</span></code>, then calculates from the first to the fourth derivative by calling <code class="docutils literal notranslate"><span class="pre">Algodiff.AD.diff</span></code> function.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">open</span> <span class="nn">Algodiff</span><span class="p">.</span><span class="nc">D</span><span class="o">;;</span>

<span class="k">let</span> <span class="n">map</span> <span class="n">f</span> <span class="n">x</span> <span class="o">=</span> <span class="nn">Vec</span><span class="p">.</span><span class="n">map</span> <span class="o">(</span><span class="k">fun</span> <span class="n">a</span> <span class="o">-&gt;</span> <span class="n">a</span> <span class="o">|&gt;</span> <span class="n">pack_flt</span> <span class="o">|&gt;</span> <span class="n">f</span> <span class="o">|&gt;</span> <span class="n">unpack_flt</span><span class="o">)</span> <span class="n">x</span><span class="o">;;</span>

<span class="c">(* calculate derivatives of f0 *)</span>
<span class="k">let</span> <span class="n">f0</span> <span class="n">x</span> <span class="o">=</span> <span class="nn">Maths</span><span class="p">.</span><span class="o">(</span><span class="n">tanh</span> <span class="n">x</span><span class="o">);;</span>
<span class="k">let</span> <span class="n">f1</span> <span class="o">=</span> <span class="n">diff</span> <span class="n">f0</span><span class="o">;;</span>
<span class="k">let</span> <span class="n">f2</span> <span class="o">=</span> <span class="n">diff</span> <span class="n">f1</span><span class="o">;;</span>
<span class="k">let</span> <span class="n">f3</span> <span class="o">=</span> <span class="n">diff</span> <span class="n">f2</span><span class="o">;;</span>
<span class="k">let</span> <span class="n">f4</span> <span class="o">=</span> <span class="n">diff</span> <span class="n">f3</span><span class="o">;;</span>

<span class="k">let</span> <span class="n">x</span> <span class="o">=</span> <span class="nn">Vec</span><span class="p">.</span><span class="n">linspace</span> <span class="o">(-</span><span class="mi">4</span><span class="o">.)</span> <span class="mi">4</span><span class="o">.</span> <span class="mi">200</span><span class="o">;;</span>
<span class="k">let</span> <span class="n">y0</span> <span class="o">=</span> <span class="n">map</span> <span class="n">f0</span> <span class="n">x</span><span class="o">;;</span>
<span class="k">let</span> <span class="n">y1</span> <span class="o">=</span> <span class="n">map</span> <span class="n">f1</span> <span class="n">x</span><span class="o">;;</span>
<span class="k">let</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">map</span> <span class="n">f2</span> <span class="n">x</span><span class="o">;;</span>
<span class="k">let</span> <span class="n">y3</span> <span class="o">=</span> <span class="n">map</span> <span class="n">f3</span> <span class="n">x</span><span class="o">;;</span>
<span class="k">let</span> <span class="n">y4</span> <span class="o">=</span> <span class="n">map</span> <span class="n">f4</span> <span class="n">x</span><span class="o">;;</span>

<span class="c">(* plot the values of all functions *)</span>
<span class="k">let</span> <span class="n">h</span> <span class="o">=</span> <span class="nn">Plot</span><span class="p">.</span><span class="n">create</span> <span class="s2">&quot;plot_021.png&quot;</span> <span class="k">in</span>
<span class="nn">Plot</span><span class="p">.</span><span class="n">set_foreground_color</span> <span class="n">h</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="o">;</span>
<span class="nn">Plot</span><span class="p">.</span><span class="n">set_background_color</span> <span class="n">h</span> <span class="mi">255</span> <span class="mi">255</span> <span class="mi">255</span><span class="o">;</span>
<span class="nn">Plot</span><span class="p">.</span><span class="n">plot</span> <span class="o">~</span><span class="n">h</span> <span class="n">x</span> <span class="n">y0</span><span class="o">;</span>
<span class="nn">Plot</span><span class="p">.</span><span class="n">plot</span> <span class="o">~</span><span class="n">h</span> <span class="n">x</span> <span class="n">y1</span><span class="o">;</span>
<span class="nn">Plot</span><span class="p">.</span><span class="n">plot</span> <span class="o">~</span><span class="n">h</span> <span class="n">x</span> <span class="n">y2</span><span class="o">;</span>
<span class="nn">Plot</span><span class="p">.</span><span class="n">plot</span> <span class="o">~</span><span class="n">h</span> <span class="n">x</span> <span class="n">y3</span><span class="o">;</span>
<span class="nn">Plot</span><span class="p">.</span><span class="n">plot</span> <span class="o">~</span><span class="n">h</span> <span class="n">x</span> <span class="n">y4</span><span class="o">;</span>
<span class="nn">Plot</span><span class="p">.</span><span class="n">output</span> <span class="n">h</span><span class="o">;;</span>
</pre></div>
</div>
<p>Start your <code class="docutils literal notranslate"><span class="pre">utop</span></code>, then load and open <code class="docutils literal notranslate"><span class="pre">Owl</span></code> library. Copy and past the code above, the generated figure will look like this.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/plot_021.png"><img alt="higher order derivatives" src="../_images/plot_021.png" style="width: 100%;" /></a>
</div>
<p>If you replace <code class="docutils literal notranslate"><span class="pre">f0</span></code> in the previous example with the following definition, then you will have another good-looking figure :)</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">let</span> <span class="n">f0</span> <span class="n">x</span> <span class="o">=</span> <span class="nn">Maths</span><span class="p">.</span><span class="o">(</span>
  <span class="k">let</span> <span class="n">y</span> <span class="o">=</span> <span class="n">exp</span> <span class="o">(</span><span class="n">neg</span> <span class="n">x</span><span class="o">)</span> <span class="k">in</span>
  <span class="o">(</span><span class="nc">F</span> <span class="mi">1</span><span class="o">.</span> <span class="o">-</span> <span class="n">y</span><span class="o">)</span> <span class="o">/</span> <span class="o">(</span><span class="nc">F</span> <span class="mi">1</span><span class="o">.</span> <span class="o">+</span> <span class="n">y</span><span class="o">)</span>
<span class="o">);;</span>
</pre></div>
</div>
<p>As you see, you can just keep calling <code class="docutils literal notranslate"><span class="pre">diff</span></code> to get higher and higher-order derivatives. E.g., <code class="docutils literal notranslate"><span class="pre">let</span> <span class="pre">g</span> <span class="pre">=</span> <span class="pre">f</span> <span class="pre">|&gt;</span> <span class="pre">diff</span> <span class="pre">|&gt;</span> <span class="pre">diff</span> <span class="pre">|&gt;</span> <span class="pre">diff</span> <span class="pre">|&gt;</span> <span class="pre">diff</span></code> will give you the fourth derivative of <code class="docutils literal notranslate"><span class="pre">f</span></code>.</p>
</div>
<div class="section" id="example-2-gradient-descent-algorithm">
<h3>Example 2: Gradient Descent Algorithm<a class="headerlink" href="#example-2-gradient-descent-algorithm" title="Permalink to this headline">¶</a></h3>
<p>Gradient Descent (GD) is a popular numerical method for calculating the optimal value for a given function. Often you need to hand craft the derivative of your function <code class="docutils literal notranslate"><span class="pre">f</span></code> before plugging into gradient descendent algorithm. With <code class="docutils literal notranslate"><span class="pre">Algodiff</span></code>, derivation can be done easily. The following several lines of code define the skeleton of GD.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">open</span> <span class="nn">Algodiff</span><span class="p">.</span><span class="nc">D</span>

<span class="k">let</span> <span class="k">rec</span> <span class="n">desc</span> <span class="o">?(</span><span class="n">eta</span><span class="o">=</span><span class="nc">F</span> <span class="mi">0</span><span class="o">.</span><span class="mi">01</span><span class="o">)</span> <span class="o">?(</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="o">)</span> <span class="n">f</span> <span class="n">x</span> <span class="o">=</span>
  <span class="k">let</span> <span class="n">g</span> <span class="o">=</span> <span class="o">(</span><span class="n">diff</span> <span class="n">f</span><span class="o">)</span> <span class="n">x</span> <span class="k">in</span>
  <span class="k">if</span> <span class="o">(</span><span class="n">unpack_flt</span> <span class="n">g</span><span class="o">)</span> <span class="o">&lt;</span> <span class="n">eps</span> <span class="k">then</span> <span class="n">x</span>
  <span class="k">else</span> <span class="n">desc</span> <span class="o">~</span><span class="n">eta</span> <span class="o">~</span><span class="n">eps</span> <span class="n">f</span> <span class="nn">Maths</span><span class="p">.</span><span class="o">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g</span><span class="o">);;</span>
</pre></div>
</div>
<p>Now let’s define a function we want to optimise, then plug it into <code class="docutils literal notranslate"><span class="pre">desc</span></code> function.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">let</span> <span class="n">f</span> <span class="n">x</span> <span class="o">=</span> <span class="nn">Maths</span><span class="p">.</span><span class="o">(</span><span class="n">sin</span> <span class="n">x</span> <span class="o">+</span> <span class="n">cos</span> <span class="n">x</span><span class="o">);;</span>

<span class="k">let</span> <span class="n">x_min</span> <span class="o">=</span> <span class="n">desc</span> <span class="n">f</span> <span class="o">(</span><span class="nc">F</span> <span class="mi">0</span><span class="o">.</span><span class="mi">1</span><span class="o">);;</span>
</pre></div>
</div>
<p>Because we started searching from <code class="docutils literal notranslate"><span class="pre">0.</span></code>, the <code class="docutils literal notranslate"><span class="pre">desc</span></code> function successfully found the local minimum at <code class="docutils literal notranslate"><span class="pre">-2.35619175250552448</span></code>. You can visually verify that by plotting it out.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">let</span> <span class="n">g</span> <span class="n">x</span> <span class="o">=</span> <span class="n">sin</span> <span class="n">x</span> <span class="o">+.</span> <span class="n">cos</span> <span class="n">x</span> <span class="k">in</span>
<span class="nn">Plot</span><span class="p">.</span><span class="n">plot_fun</span> <span class="n">g</span> <span class="o">(-</span><span class="mi">5</span><span class="o">.)</span> <span class="mi">5</span><span class="o">.;;</span>
</pre></div>
</div>
</div>
<div class="section" id="example-3-newton-s-algorithm">
<h3>Example 3: Newton’s Algorithm<a class="headerlink" href="#example-3-newton-s-algorithm" title="Permalink to this headline">¶</a></h3>
<p>Newton’s method is a root-finding algorithm by successively searching for better approximation of the root. The Newton’s method converges faster than gradient descent. The following implementation calculates the exact hessian of <code class="docutils literal notranslate"><span class="pre">f</span></code> which in practice is very expensive operation.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">open</span> <span class="nn">Algodiff</span><span class="p">.</span><span class="nc">D</span>

<span class="k">let</span> <span class="k">rec</span> <span class="n">newton</span> <span class="o">?(</span><span class="n">eta</span><span class="o">=</span><span class="nc">F</span> <span class="mi">0</span><span class="o">.</span><span class="mi">01</span><span class="o">)</span> <span class="o">?(</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="o">)</span> <span class="n">f</span> <span class="n">x</span> <span class="o">=</span>
  <span class="k">let</span> <span class="n">g</span><span class="o">,</span> <span class="n">h</span> <span class="o">=</span> <span class="o">(</span><span class="n">gradhessian</span> <span class="n">f</span><span class="o">)</span> <span class="n">x</span> <span class="k">in</span>
  <span class="k">if</span> <span class="o">(</span><span class="nn">Maths</span><span class="p">.</span><span class="n">l2norm</span> <span class="n">g</span> <span class="o">|&gt;</span> <span class="n">unpack_flt</span><span class="o">)</span> <span class="o">&lt;</span> <span class="n">eps</span> <span class="k">then</span> <span class="n">x</span>
  <span class="k">else</span> <span class="n">newton</span> <span class="o">~</span><span class="n">eta</span> <span class="o">~</span><span class="n">eps</span> <span class="n">f</span> <span class="nn">Maths</span><span class="p">.</span><span class="o">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g</span> <span class="o">*@</span> <span class="o">(</span><span class="n">inv</span> <span class="n">h</span><span class="o">));;</span>
</pre></div>
</div>
<p>Now we can apply <code class="docutils literal notranslate"><span class="pre">newton</span></code> to find the extreme value of <code class="docutils literal notranslate"><span class="pre">Maths.(cos</span> <span class="pre">x</span> <span class="pre">|&gt;</span> <span class="pre">sum)</span></code>.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">let</span> <span class="o">_</span> <span class="o">=</span>
  <span class="k">let</span> <span class="n">f</span> <span class="n">x</span> <span class="o">=</span> <span class="nn">Maths</span><span class="p">.</span><span class="o">(</span><span class="n">cos</span> <span class="n">x</span> <span class="o">|&gt;</span> <span class="n">sum</span><span class="o">)</span> <span class="k">in</span>
  <span class="k">let</span> <span class="n">y</span> <span class="o">=</span> <span class="n">newton</span> <span class="n">f</span> <span class="o">(</span><span class="nn">Mat</span><span class="p">.</span><span class="n">uniform</span> <span class="mi">1</span> <span class="mi">2</span><span class="o">)</span> <span class="k">in</span>
  <span class="nn">Mat</span><span class="p">.</span><span class="n">print</span> <span class="n">y</span><span class="o">;;</span>
</pre></div>
</div>
</div>
<div class="section" id="example-4-backpropagation-in-neural-network">
<h3>Example 4: Backpropagation in Neural Network<a class="headerlink" href="#example-4-backpropagation-in-neural-network" title="Permalink to this headline">¶</a></h3>
<p>Now let’s talk about the hyped neural network. Backpropagation is the core of all neural networks, actually it is just a special case of reverse mode AD. Therefore, we can write up the backpropagation algorithm from scratch easily with the help of <code class="docutils literal notranslate"><span class="pre">Algodiff</span></code> module.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">let</span> <span class="n">backprop</span> <span class="n">nn</span> <span class="n">eta</span> <span class="n">x</span> <span class="n">y</span> <span class="o">=</span>
  <span class="k">let</span> <span class="n">t</span> <span class="o">=</span> <span class="n">tag</span> <span class="bp">()</span> <span class="k">in</span>
  <span class="nn">Array</span><span class="p">.</span><span class="n">iter</span> <span class="o">(</span><span class="k">fun</span> <span class="n">l</span> <span class="o">-&gt;</span>
    <span class="n">l</span><span class="o">.</span><span class="n">w</span> <span class="o">&lt;-</span> <span class="n">make_reverse</span> <span class="n">l</span><span class="o">.</span><span class="n">w</span> <span class="n">t</span><span class="o">;</span>
    <span class="n">l</span><span class="o">.</span><span class="n">b</span> <span class="o">&lt;-</span> <span class="n">make_reverse</span> <span class="n">l</span><span class="o">.</span><span class="n">b</span> <span class="n">t</span><span class="o">;</span>
  <span class="o">)</span> <span class="n">nn</span><span class="o">.</span><span class="n">layers</span><span class="o">;</span>
  <span class="k">let</span> <span class="n">loss</span> <span class="o">=</span> <span class="nn">Maths</span><span class="p">.</span><span class="o">(</span><span class="n">cross_entropy</span> <span class="n">y</span> <span class="o">(</span><span class="n">run_network</span> <span class="n">x</span> <span class="n">nn</span><span class="o">)</span> <span class="o">/</span> <span class="o">(</span><span class="nc">F</span> <span class="o">(</span><span class="nn">Mat</span><span class="p">.</span><span class="n">row_num</span> <span class="n">y</span> <span class="o">|&gt;</span> <span class="n">float_of_int</span><span class="o">)))</span> <span class="k">in</span>
  <span class="n">reverse_prop</span> <span class="o">(</span><span class="nc">F</span> <span class="mi">1</span><span class="o">.)</span> <span class="n">loss</span><span class="o">;</span>
  <span class="nn">Array</span><span class="p">.</span><span class="n">iter</span> <span class="o">(</span><span class="k">fun</span> <span class="n">l</span> <span class="o">-&gt;</span>
    <span class="n">l</span><span class="o">.</span><span class="n">w</span> <span class="o">&lt;-</span> <span class="nn">Maths</span><span class="p">.</span><span class="o">((</span><span class="n">primal</span> <span class="n">l</span><span class="o">.</span><span class="n">w</span><span class="o">)</span> <span class="o">-</span> <span class="o">(</span><span class="n">eta</span> <span class="o">*</span> <span class="o">(</span><span class="n">adjval</span> <span class="n">l</span><span class="o">.</span><span class="n">w</span><span class="o">)))</span> <span class="o">|&gt;</span> <span class="n">primal</span><span class="o">;</span>
    <span class="n">l</span><span class="o">.</span><span class="n">b</span> <span class="o">&lt;-</span> <span class="nn">Maths</span><span class="p">.</span><span class="o">((</span><span class="n">primal</span> <span class="n">l</span><span class="o">.</span><span class="n">b</span><span class="o">)</span> <span class="o">-</span> <span class="o">(</span><span class="n">eta</span> <span class="o">*</span> <span class="o">(</span><span class="n">adjval</span> <span class="n">l</span><span class="o">.</span><span class="n">b</span><span class="o">)))</span> <span class="o">|&gt;</span> <span class="n">primal</span><span class="o">;</span>
  <span class="o">)</span> <span class="n">nn</span><span class="o">.</span><span class="n">layers</span><span class="o">;</span>
  <span class="n">loss</span> <span class="o">|&gt;</span> <span class="n">unpack_flt</span>
</pre></div>
</div>
<p>Yes, we just used only 13 lines of code to implement the backpropagation. Actually, with some extra coding, we can make a smart application to recognise handwritten digits. E.g., running the application will give you the following prediction on handwritten digit <code class="docutils literal notranslate"><span class="pre">6</span></code>. The code has been included in Owl’s example and you can find the complete example in <a class="reference external" href="https://github.com/ryanrhymes/owl/blob/master/examples/backprop.ml">backprop.ml</a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/plot_034.png"><img alt="mnist experiment" src="../_images/plot_034.png" style="width: 100%;" /></a>
</div>
</div>
<div class="section" id="example-5-computation-graph-of-simple-functions">
<h3>Example 5: Computation Graph of Simple Functions<a class="headerlink" href="#example-5-computation-graph-of-simple-functions" title="Permalink to this headline">¶</a></h3>
<p>Backward mode generates and maintains a computation graph in order to back propagate the error. The computation graph is very helpful in both debugging and understanding the characteristic of your numerical functions. Owl provides two functions to facilitate you in generating computation graphs.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">val</span> <span class="n">to_trace</span><span class="o">:</span> <span class="n">t</span> <span class="kt">list</span> <span class="o">-&gt;</span> <span class="kt">string</span>
<span class="c">(* print out the trace in human-readable format *)</span>

<span class="k">val</span> <span class="n">to_dot</span> <span class="o">:</span> <span class="n">tlist</span> <span class="o">-&gt;</span> <span class="kt">string</span>
<span class="c">(* print out the computation graph in dot format *)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">to_trace</span></code> is useful when the graph is small and you can print it out on the terminal then observe it directly. <code class="docutils literal notranslate"><span class="pre">to_dot</span></code> is more useful when the graph grows bigger since you can use specialised visualisation tools to generate professional figures, such as Graphviz.</p>
<p>In the following, I will showcase several computation graphs. However, I will skip the details of how to generate these graphs since you can find out in the <a class="reference external" href="https://github.com/ryanrhymes/owl/blob/master/examples/computation_graph.ml">computation_graph.ml</a>.</p>
<p>Let’s start with a simple function as below.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">let</span> <span class="n">f</span> <span class="n">x</span> <span class="n">y</span> <span class="o">=</span> <span class="nn">Maths</span><span class="p">.</span><span class="o">((</span><span class="n">x</span> <span class="o">*</span> <span class="n">sin</span> <span class="o">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">x</span><span class="o">)</span> <span class="o">+</span> <span class="o">(</span> <span class="nc">F</span> <span class="mi">1</span><span class="o">.</span> <span class="o">*</span> <span class="n">sqrt</span> <span class="n">x</span><span class="o">)</span> <span class="o">/</span> <span class="nc">F</span> <span class="mi">7</span><span class="o">.)</span> <span class="o">*</span> <span class="o">(</span><span class="n">relu</span> <span class="n">y</span><span class="o">)</span> <span class="o">|&gt;</span> <span class="n">sum</span><span class="o">)</span>
</pre></div>
</div>
<p>The generated computation graph looks like this.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/plot_028.png"><img alt="computation graph of simple functions" src="../_images/plot_028.png" style="width: 507.0px; height: 611.0px;" /></a>
</div>
</div>
<div class="section" id="example-6-computation-graph-of-vgg-like-neural-network">
<h3>Example 6: Computation Graph of VGG-like Neural Network<a class="headerlink" href="#example-6-computation-graph-of-vgg-like-neural-network" title="Permalink to this headline">¶</a></h3>
<p>Let’s define a VGG-like neural network as below.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">open</span> <span class="nn">Neural</span><span class="p">.</span><span class="nc">S</span>
<span class="k">open</span> <span class="nn">Neural</span><span class="p">.</span><span class="nn">S</span><span class="p">.</span><span class="nc">Graph</span>

<span class="k">let</span> <span class="n">make_network</span> <span class="n">input_shape</span> <span class="o">=</span>
  <span class="n">input</span> <span class="n">input_shape</span>
  <span class="o">|&gt;</span> <span class="n">normalisation</span> <span class="o">~</span><span class="n">decay</span><span class="o">:</span><span class="mi">0</span><span class="o">.</span><span class="mi">9</span>
  <span class="o">|&gt;</span> <span class="n">conv2d</span> <span class="o">[|</span><span class="mi">3</span><span class="o">;</span><span class="mi">3</span><span class="o">;</span><span class="mi">3</span><span class="o">;</span><span class="mi">32</span><span class="o">|]</span> <span class="o">[|</span><span class="mi">1</span><span class="o">;</span><span class="mi">1</span><span class="o">|]</span> <span class="o">~</span><span class="n">act_typ</span><span class="o">:</span><span class="nn">Activation</span><span class="p">.</span><span class="nc">Relu</span>
  <span class="o">|&gt;</span> <span class="n">conv2d</span> <span class="o">[|</span><span class="mi">3</span><span class="o">;</span><span class="mi">3</span><span class="o">;</span><span class="mi">32</span><span class="o">;</span><span class="mi">32</span><span class="o">|]</span> <span class="o">[|</span><span class="mi">1</span><span class="o">;</span><span class="mi">1</span><span class="o">|]</span> <span class="o">~</span><span class="n">act_typ</span><span class="o">:</span><span class="nn">Activation</span><span class="p">.</span><span class="nc">Relu</span> <span class="o">~</span><span class="n">padding</span><span class="o">:</span><span class="nc">VALID</span>
  <span class="o">|&gt;</span> <span class="n">max_pool2d</span> <span class="o">[|</span><span class="mi">2</span><span class="o">;</span><span class="mi">2</span><span class="o">|]</span> <span class="o">[|</span><span class="mi">2</span><span class="o">;</span><span class="mi">2</span><span class="o">|]</span> <span class="o">~</span><span class="n">padding</span><span class="o">:</span><span class="nc">VALID</span>
  <span class="o">|&gt;</span> <span class="n">dropout</span> <span class="mi">0</span><span class="o">.</span><span class="mi">1</span>
  <span class="o">|&gt;</span> <span class="n">conv2d</span> <span class="o">[|</span><span class="mi">3</span><span class="o">;</span><span class="mi">3</span><span class="o">;</span><span class="mi">32</span><span class="o">;</span><span class="mi">64</span><span class="o">|]</span> <span class="o">[|</span><span class="mi">1</span><span class="o">;</span><span class="mi">1</span><span class="o">|]</span> <span class="o">~</span><span class="n">act_typ</span><span class="o">:</span><span class="nn">Activation</span><span class="p">.</span><span class="nc">Relu</span>
  <span class="o">|&gt;</span> <span class="n">conv2d</span> <span class="o">[|</span><span class="mi">3</span><span class="o">;</span><span class="mi">3</span><span class="o">;</span><span class="mi">64</span><span class="o">;</span><span class="mi">64</span><span class="o">|]</span> <span class="o">[|</span><span class="mi">1</span><span class="o">;</span><span class="mi">1</span><span class="o">|]</span> <span class="o">~</span><span class="n">act_typ</span><span class="o">:</span><span class="nn">Activation</span><span class="p">.</span><span class="nc">Relu</span> <span class="o">~</span><span class="n">padding</span><span class="o">:</span><span class="nc">VALID</span>
  <span class="o">|&gt;</span> <span class="n">max_pool2d</span> <span class="o">[|</span><span class="mi">2</span><span class="o">;</span><span class="mi">2</span><span class="o">|]</span> <span class="o">[|</span><span class="mi">2</span><span class="o">;</span><span class="mi">2</span><span class="o">|]</span> <span class="o">~</span><span class="n">padding</span><span class="o">:</span><span class="nc">VALID</span>
  <span class="o">|&gt;</span> <span class="n">dropout</span> <span class="mi">0</span><span class="o">.</span><span class="mi">1</span>
  <span class="o">|&gt;</span> <span class="n">fully_connected</span> <span class="mi">512</span> <span class="o">~</span><span class="n">act_typ</span><span class="o">:</span><span class="nn">Activation</span><span class="p">.</span><span class="nc">Relu</span>
  <span class="o">|&gt;</span> <span class="n">linear</span> <span class="mi">10</span> <span class="o">~</span><span class="n">act_typ</span><span class="o">:</span><span class="nn">Activation</span><span class="p">.</span><span class="nc">Softmax</span>
  <span class="o">|&gt;</span> <span class="n">get_network</span>
</pre></div>
</div>
<p>The computation graph for this neural network become a bit more complicated now.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/plot_029.png"><img alt="computation graph of VGG" src="../_images/plot_029.png" style="width: 100%;" /></a>
</div>
</div>
<div class="section" id="example-7-computation-graph-of-lstm-network">
<h3>Example 7: Computation Graph of LSTM Network<a class="headerlink" href="#example-7-computation-graph-of-lstm-network" title="Permalink to this headline">¶</a></h3>
<p>How about LSTM network? The following definition seems much lighter than convolutional neural network in the previous example.</p>
<div class="highlight-ocaml notranslate"><div class="highlight"><pre><span></span><span class="k">open</span> <span class="nn">Neural</span><span class="p">.</span><span class="nc">S</span>
<span class="k">open</span> <span class="nn">Neural</span><span class="p">.</span><span class="nn">S</span><span class="p">.</span><span class="nc">Graph</span>

<span class="k">let</span> <span class="n">make_network</span> <span class="n">wndsz</span> <span class="n">vocabsz</span> <span class="o">=</span>
  <span class="n">input</span> <span class="o">[|</span><span class="n">wndsz</span><span class="o">|]</span>
  <span class="o">|&gt;</span> <span class="n">embedding</span> <span class="n">vocabsz</span> <span class="mi">40</span>
  <span class="o">|&gt;</span> <span class="n">lstm</span> <span class="mi">128</span>
  <span class="o">|&gt;</span> <span class="n">linear</span> <span class="mi">512</span> <span class="o">~</span><span class="n">act_typ</span><span class="o">:</span><span class="nn">Activation</span><span class="p">.</span><span class="nc">Relu</span>
  <span class="o">|&gt;</span> <span class="n">linear</span> <span class="n">vocabsz</span> <span class="o">~</span><span class="n">act_typ</span><span class="o">:</span><span class="nn">Activation</span><span class="p">.</span><span class="nc">Softmax</span>
  <span class="o">|&gt;</span> <span class="n">get_network</span>
</pre></div>
</div>
<p>However, the generated computation graph is way more complicated due to LSTM’s internal recurrent structure. You can download the <a class="reference external" href="https://raw.githubusercontent.com/wiki/ryanrhymes/owl/image/plot_030.pdf">[PDF file 1]</a> for better image quality.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/plot_030.png"><img alt="computation graph of lstm" src="../_images/plot_030.png" style="width: 100%;" /></a>
</div>
</div>
<div class="section" id="example-8-computation-graph-of-google-s-inception">
<h3>Example 8: Computation Graph of Google’s Inception<a class="headerlink" href="#example-8-computation-graph-of-google-s-inception" title="Permalink to this headline">¶</a></h3>
<p>If the computation graph above hasn’t scared you yet, here is another one generated from Google’s Inception network for image classification. I will not paste the code here since the definition of the network per se is already quite complicated. You can use Owl’s zoo system <code class="docutils literal notranslate"><span class="pre">#zoo</span> <span class="pre">&quot;6dfed11c521fb2cd286f2519fb88d3bf&quot;</span></code>.</p>
<p>The image below is too small to check details, please download the <a class="reference external" href="https://raw.githubusercontent.com/wiki/ryanrhymes/owl/image/plot_031.pdf">[PDF file 2]</a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/plot_031.png"><img alt="computation graph of inception" src="../_images/plot_031.png" style="width: 100%;" /></a>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &copy;2018, <a href="http://www.cl.cam.ac.uk/~lw525/">Liang Wang</a>   | <a href="http://www.cl.cam.ac.uk/">Computer Lab, University of Cambridge</a>.
      
      |
      <a href="../_sources/chapter/algodiff.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>